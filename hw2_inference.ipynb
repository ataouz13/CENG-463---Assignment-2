{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KtGQGHm0Noea"},"outputs":[],"source":["!pip install datasets"]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"vAGm5O58NrSy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pt1dGuDGNtJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n","import torch\n","import matplotlib.pyplot as plt\n","from datasets import Dataset\n","from tqdm import tqdm\n","import random"],"metadata":{"id":"oMa7xLhGje-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgEjnHx1N5O8"},"outputs":[],"source":["task = 2\n","\n","data_path = \"drive/MyDrive/463_hw2_data/\"\n","if task == 1:\n","    data_path = data_path + \"test_data_orientation.tsv\"\n","else:\n","    data_path = data_path + \"train_data_power.tsv\"\n","\n","\n","data = pd.read_csv(data_path, sep='\\t')\n","\n","data = data.dropna(subset=[\"text_en\"])  # Remove rows where text_en is NaN\n","data = data[data[\"text_en\"].str.strip() != \"\"]  # Remove rows where text_en is empty or whitespace\n","\n","data = data.reset_index(drop=True)\n","\n","sample_size = 1000\n","seed = 42\n","data = data.sample(n=sample_size, random_state=seed).reset_index(drop=True)"]},{"cell_type":"code","source":["# Prepare data\n","data_en = {\"text\": data[\"text_en\"], \"task\": [task] * len(data)}\n","data_original = {\"text\": data[\"text\"], \"task\": [task] * len(data)}\n","\n","# Create a Dataset\n","dataset_en = Dataset.from_dict(data_en)\n","dataset_original = Dataset.from_dict(data_original)\n","\n","# Define a function to generate prompts\n","def create_prompt(text, task):\n","    if task == 1:\n","        return f\"Is the speaker's party leaning left (0) or right (1)?\\nText: {text}\\nAnswer, only say 0 or 1:\"\n","    elif task == 2:\n","        return f\"Is the speaker's party governing (0) or in opposition (1)?\\nText: {text}\\nAnswer, only say 0 or 1:\"\n","\n","def generate_prompts(example):\n","    text = example[\"text\"]\n","    if example[\"task\"] == 1:\n","        example[\"prompt\"] = f\"Is the speaker's party leaning left (0) or right (1)?\\nText: {text}\\nAnswer:\"\n","    elif example[\"task\"] == 2:\n","        example[\"prompt\"] = f\"Is the speaker's party governing (0) or in opposition (1)?\\nText: {text}\\nAnswer:\"\n","    return example\n","\n","# Apply the function to create prompts\n","dataset_en = dataset_en.map(generate_prompts)\n","dataset_original = dataset_original.map(generate_prompts)"],"metadata":{"id":"Ykz-R3THOvsl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the pre-trained causal language model\n","model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n","causal_pipeline = pipeline(\"text-generation\", model=model_name, device=0, torch_dtype=torch.float16, batch_size = 4)\n","\n","# Ensure the tokenizer used in causal_pipeline has a pad_token_id\n","causal_pipeline.tokenizer.pad_token_id = causal_pipeline.tokenizer.eos_token_id\n","\n","# Ensure the model is on the GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","causal_pipeline.model.to(device)\n"],"metadata":{"id":"jztZ3J-8t1Pt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define batch size\n","batch_size = 4\n","\n","def inference(dataset):\n","    num_batches = len(dataset) // batch_size + (len(dataset) % batch_size > 0)\n","    results = []\n","\n","    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing Batches\", unit=\"batch\"):\n","        batch_prompts = dataset[\"prompt\"][i:i + batch_size]  # Get a batch of prompts\n","        batch_results = causal_pipeline(batch_prompts, max_new_tokens=100, do_sample=True, temperature=0.8, top_p=0.8, pad_token_id=128001)\n","        results.extend(batch_results)  # Append results\n","\n","    return results\n","\n","print(\"Inference on English text\")\n","results_en = inference(dataset_en)\n","\n","print(\"Inference on original text\")\n","results_original = inference(dataset_original)"],"metadata":{"id":"pyjAkSolukgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process results\n","def make_predictions(results):\n","    predictions = []\n","    for output in results:\n","        prediction = output[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n","        prediction.lower()\n","        if task == 1:\n","            if (\"0\" in prediction) or (\"left\" in prediction):\n","                predictions.append(0)\n","            elif (\"1\" in prediction) or (\"right\" in prediction):\n","                predictions.append(1)\n","            else:\n","                predictions.append(random.randint(0,1))\n","        elif task == 2:\n","            if (\"0\" in prediction) or (\"governing\" in prediction):\n","                predictions.append(0)\n","            elif (\"1\" in prediction) or (\"opposition\" in prediction):\n","                predictions.append(1)\n","\n","            else:\n","                predictions.append(random.randint(0,1))\n","        else:\n","            print(\"Invalid task\")\n","\n","    return predictions\n","\n","\n","predictions_en = make_predictions(results_en)\n","predictions_original = make_predictions(results_original)"],"metadata":{"id":"KhRosqmtk46K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the results\n","def evaluate(true_labels, predictions, task_name, text_type):\n","    print(f\"\\nEvaluation for {task_name} ({text_type}):\")\n","    print(classification_report(true_labels, predictions, zero_division=0))\n","\n","    # Compute the confusion matrix\n","    cm = confusion_matrix(true_labels, predictions)\n","\n","    # Plot the confusion matrix\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n","    disp.plot(cmap=plt.cm.Blues, values_format=\"d\")\n","    plt.title(f\"Confusion Matrix for {task_name} ({text_type})\")\n","    plt.show()\n","\n","true_labels = data[\"label\"]\n","if task == 1:\n","    task_name = \"Task 1 - Political Ideology\"\n","else:\n","    task_name = \"Task 2 - Government Status\"\n","\n","evaluate(true_labels, predictions_en, task_name, \"English\")\n","evaluate(true_labels, predictions_original, task_name, \"Original Language\")"],"metadata":{"id":"_so1_VVhu_ME"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyN68A3BEzooRdfra5mcva0t"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}