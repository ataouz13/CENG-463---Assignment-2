{"cells":[{"cell_type":"code","execution_count":1,"id":"8wZaHuC2Yazp","metadata":{"executionInfo":{"elapsed":17411,"status":"ok","timestamp":1736073272151,"user":{"displayName":"Ata Oğuz Tanrıkulu","userId":"13034422966169063166"},"user_tz":-180},"id":"8wZaHuC2Yazp"},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AdamW\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, classification_report\n","from sklearn.model_selection import train_test_split\n","from transformers import XLMRobertaTokenizer, XLMRobertaModel\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from torch.optim.lr_scheduler import CosineAnnealingLR"]},{"cell_type":"code","execution_count":null,"id":"gzs4vcjSYr6g","metadata":{"id":"gzs4vcjSYr6g"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"W0KIQbxDLl8N","metadata":{"executionInfo":{"elapsed":34247,"status":"ok","timestamp":1736073334259,"user":{"displayName":"Ata Oğuz Tanrıkulu","userId":"13034422966169063166"},"user_tz":-180},"id":"W0KIQbxDLl8N"},"outputs":[],"source":["task = 1\n","\n","data_path = \"drive/MyDrive/463_hw2_data/\"\n","\n","if task == 1:\n","    train_path = data_path + \"train_data_orientation.tsv\"\n","    test_path = data_path + \"test_data_orientation.tsv\"\n","    text_type = \"text_en\"\n","elif task == 2:\n","    train_path = data_path + \"train_data_power.tsv\"\n","    test_path = data_path + \"test_data_power.tsv\"\n","    text_type = \"text\"\n","else:\n","    raise ValueError(\"Invalid task number\")\n","\n","\n","train_df = pd.read_csv(train_path, sep='\\t')\n","test_df = pd.read_csv(test_path, sep='\\t')\n","\n","# Drop rows with missing or empty text_en\n","train_df = train_df.dropna(subset=[\"text_en\"])  # Remove rows where text_en is NaN\n","train_df = train_df[train_df[\"text_en\"].str.strip() != \"\"]  # Remove rows where text_en is empty or whitespace\n","\n","test_df = test_df.dropna(subset=[\"text_en\"])  # Remove rows where text_en is NaN\n","test_df = test_df[test_df[\"text_en\"].str.strip() != \"\"]  # Remove rows where text_en is empty or whitespace\n","\n","# Reset index after filtering\n","train_df = train_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":4,"id":"2P4SN44WYtV3","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1736073334259,"user":{"displayName":"Ata Oğuz Tanrıkulu","userId":"13034422966169063166"},"user_tz":-180},"id":"2P4SN44WYtV3"},"outputs":[],"source":["class CustomXLMRobertaModel(nn.Module):\n","    def __init__(self, pretrained_model_name=\"FacebookAI/xlm-roberta-base\", num_additional_features=1, output_size=2):\n","        super(CustomXLMRobertaModel, self).__init__()\n","        self.tokenizer = XLMRobertaTokenizer.from_pretrained(pretrained_model_name)\n","        self.xlm_roberta = XLMRobertaModel.from_pretrained(pretrained_model_name)\n","\n","        # Freeze all layers except the last few\n","        for param in self.xlm_roberta.parameters():\n","            param.requires_grad = False\n","        for param in self.xlm_roberta.encoder.layer[-8:].parameters():\n","            param.requires_grad = True\n","\n","        hidden_size = self.xlm_roberta.config.hidden_size + num_additional_features\n","\n","        self.fc1 = nn.Linear(hidden_size, 512)\n","        self.sigmoid = nn.Sigmoid()\n","        self.dropout = nn.Dropout(0.5)\n","\n","        num_heads = 8\n","        self.self_attention = nn.MultiheadAttention(embed_dim=512, num_heads=num_heads, batch_first=True)\n","\n","        # Fully connected layers\n","\n","        self.fc2 = nn.Linear(512, 128)\n","        self.tanh = nn.Tanh()\n","        self.fc3 = nn.Linear(128, 64)\n","        self.relu = nn.ReLU()\n","        self.fc4 = nn.Linear(64,output_size)  # Output size = 2\n","\n","    def forward(self, input_ids, attention_mask, additional_features):\n","        # Step 1: XLM-RoBERTa processing\n","        outputs = self.xlm_roberta(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_output = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token representation\n","\n","        # Step 2: Concatenate additional features\n","        combined_features = torch.cat((cls_output, additional_features), dim=1)  # Combine CLS and additional features\n","\n","        # Step 3: Pass through the first fully connected layer\n","        fc1_output = self.fc1(combined_features)\n","        fc1_output = self.sigmoid(fc1_output)  # Apply activation\n","        fc1_output = self.dropout(fc1_output)  # Apply dropout\n","\n","        # Step 4: Prepare for self-attention (add sequence dimension)\n","        fc1_output = fc1_output.unsqueeze(1)\n","\n","        # Step 5: Pass through the self-attention layer\n","        attn_output, _ = self.self_attention(fc1_output, fc1_output, fc1_output)\n","\n","        # Remove sequence dimension\n","        attn_output = attn_output.squeeze(1)\n","\n","        # Step 6: Pass through the remaining fully connected layers\n","        x = self.fc2(attn_output)\n","        x = self.tanh(x)  # Apply activation\n","\n","        x = self.fc3(x)\n","        x = self.relu(x)  # Apply activation\n","\n","        x = self.fc4(x)\n","        x = torch.softmax(x, dim=1)\n","\n","        return x\n","\n","class ParliamentaryDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length=128):\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        # Extract text and additional features\n","        text = self.data.loc[idx, text_type]\n","        additional_features = {\n","            \"id\": self.data.loc[idx, \"id\"],\n","            \"speaker\": self.data.loc[idx, \"speaker\"],\n","            \"sex\": self.data.loc[idx, \"sex\"]\n","        }\n","        label = self.data.loc[idx, \"label\"]\n","\n","        # Tokenize text\n","        tokenized = self.tokenizer(\n","            text,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        sex_encoded = 1 if additional_features[\"sex\"] == \"M\" else 0 if additional_features[\"sex\"] == \"F\" else -1\n","\n","        # Combine encoded features into a tensor\n","        extra_features = torch.tensor([sex_encoded], dtype=torch.float)\n","\n","        return {\n","            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n","            \"additional_features\": extra_features,\n","            \"label\": torch.tensor(label, dtype=torch.float)\n","        }\n","\n","def initialize_weights(layer):\n","    if isinstance(layer, nn.Linear):\n","        nn.init.xavier_uniform_(layer.weight)\n","        nn.init.zeros_(layer.bias)\n"]},{"cell_type":"code","execution_count":null,"id":"oZIq1J9gNxpl","metadata":{"id":"oZIq1J9gNxpl"},"outputs":[],"source":["# Define the sampling percentage\n","sampling_percentage = 1.0\n","batch_size = 64\n","max_length = 512\n","max_length = min(max_length,512)\n","pretrained_model_name=\"FacebookAI/xlm-roberta-base\"\n","\n","# Randomly sample a percentage of the training data\n","sampled_train_df = train_df.sample(frac=sampling_percentage, random_state=42).reset_index(drop=True)\n","sampled_train_df, val_df = train_test_split(sampled_train_df, test_size=0.09, random_state=42)\n","sampled_train_df.reset_index(drop=True, inplace=True)\n","val_df.reset_index(drop=True, inplace=True)\n","\n","# Create the dataset and DataLoader with the sampled data\n","train_dataset = ParliamentaryDataset(sampled_train_df, XLMRobertaTokenizer.from_pretrained(pretrained_model_name),max_length=max_length)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataset = ParliamentaryDataset(test_df, XLMRobertaTokenizer.from_pretrained(pretrained_model_name),max_length=max_length)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_dataset = ParliamentaryDataset(val_df, XLMRobertaTokenizer.from_pretrained(pretrained_model_name),max_length=max_length)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"bv9Cd15dIqAz","metadata":{"id":"bv9Cd15dIqAz"},"outputs":[],"source":["# Count occurrences of each class\n","class_counts = [sampled_train_df[\"label\"].value_counts()[0], sampled_train_df[\"label\"].value_counts()[1]]\n","total_samples = class_counts[0] + class_counts[1]\n","\n","# Calculate weights inversely proportional to class frequencies\n","class_weights = [class_counts[1], class_counts[0]]/total_samples\n","\n","# Convert to tensor\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","class_weights_tensor = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float).to(device)\n","print(class_weights_tensor)\n","criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)"]},{"cell_type":"code","execution_count":null,"id":"OH9THORJdU38","metadata":{"id":"OH9THORJdU38"},"outputs":[],"source":["reload = 1\n","num_epochs = 30\n","model_name = \"drive/MyDrive/463_hw2_data/model_task\" + str(task) + \".pth\"\n","\n","model = CustomXLMRobertaModel()\n","optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5,weight_decay=0.01)\n","scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n","\n","if reload == 0:\n","    model.apply(initialize_weights)\n","else:\n","    model = torch.load(model_name)\n"]},{"cell_type":"code","execution_count":null,"id":"Pni0aPxpdXf1","metadata":{"id":"Pni0aPxpdXf1"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    # Training phase\n","    model.train()\n","    total_train_loss = 0\n","    all_train_preds = []\n","    all_train_labels = []\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", unit=\"batch\")\n","\n","    for batch in train_loader_tqdm:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        additional_features = batch[\"additional_features\"].to(device)\n","        labels = batch[\"label\"].long().to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask, additional_features)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","        total_train_loss += loss.item()\n","        preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n","        all_train_preds.extend(preds)\n","        all_train_labels.extend(labels.detach().cpu().numpy())\n","\n","        train_loader_tqdm.set_postfix({\"Batch Loss\": loss.item()})\n","\n","    avg_train_loss = total_train_loss / len(train_loader)\n","    train_balanced_acc = balanced_accuracy_score(all_train_labels, all_train_preds)\n","\n","    print(f\"Training Loss: {avg_train_loss:.4f}\")\n","    print(f\"Training Balanced Accuracy: {train_balanced_acc:.4f}\")\n","    print(classification_report(all_train_labels, all_train_preds, zero_division=0))\n","\n","    # Validation phase\n","    model.eval()\n","    total_val_loss = 0\n","    all_val_preds = []\n","    all_val_labels = []\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            additional_features = batch[\"additional_features\"].to(device)\n","            labels = batch[\"label\"].long().to(device)\n","\n","            outputs = model(input_ids, attention_mask, additional_features)\n","            loss = criterion(outputs, labels)\n","\n","            total_val_loss += loss.item()\n","            preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n","            all_val_preds.extend(preds)\n","            all_val_labels.extend(labels.detach().cpu().numpy())\n","\n","    avg_val_loss = total_val_loss / len(val_loader)\n","    val_balanced_acc = balanced_accuracy_score(all_val_labels, all_val_preds)\n","\n","    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"Validation Balanced Accuracy: {val_balanced_acc:.4f}\")\n","    print(classification_report(all_val_labels, all_val_preds, zero_division=0))\n","\n","    # Step the scheduler\n","    scheduler.step(avg_val_loss)\n","    print(f\"Learning Rate after Epoch {epoch+1}: {scheduler._last_lr[0]:.3e}\")\n","    print(\"---------------------------------------------------------------------------\")\n","\n","    torch.save(model, model_name)"]},{"cell_type":"code","execution_count":null,"id":"MNVEx6fCgKB-","metadata":{"id":"MNVEx6fCgKB-"},"outputs":[],"source":["# Testing loop\n","model.eval()  # Set the model to evaluation mode\n","all_preds = []\n","all_labels = []\n","\n","print(\"Evaluating...\")\n","test_loader_tqdm = tqdm(test_loader, desc=\"Processing Batches\", unit=\"batch\")  # Wrap test_loader with tqdm\n","\n","with torch.no_grad():  # No need to compute gradients during testing\n","    for batch_idx, batch in enumerate(test_loader_tqdm):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        additional_features = batch[\"additional_features\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        outputs = model(input_ids, attention_mask, additional_features).squeeze(1)\n","        preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n","        all_preds.extend(preds)\n","        all_labels.extend(labels.detach().cpu().numpy())\n","\n","conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","# Plot confusion matrix\n","if task == 1:\n","    class_names = [\"Left-0\", \"Right-1\"]\n","elif task == 2:\n","    class_names = [\"Governing-0\", \"Opposition-1\"]\n","else:\n","    raise ValueError(\"Invalid task number\")\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n","disp.plot(cmap=plt.cm.Blues)\n","plt.title(\"Confusion Matrix\")\n","plt.show()\n","\n","print(classification_report(all_labels, all_preds, zero_division=0))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":5}